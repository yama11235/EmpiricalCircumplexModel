#!/bin/bash
set -euo pipefail

SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
TRAIN_SCRIPT="${SCRIPT_DIR}/train_emolit.sh"

if [[ ! -f "${TRAIN_SCRIPT}" ]]; then
  echo "train.sh not found at ${TRAIN_SCRIPT}" >&2
  exit 1
fi

# MODEL_NAME=("mixedbread-ai/mxbai-embed-large-v1")
# MODEL_ID=("mixedbread-ai_mxbai-embed-large-v1")
# MODEL_NAME=("Qwen/Qwen3-Embedding-4B")
# MODEL_ID=("Qwen_Qwen3-Embedding-4B")
# MODEL_NAME=("meta-llama/Llama-3.2-3B")
# MODEL_ID=("meta-llama_Llama-3.2-3B")
# MODEL_NAME=("nvidia/llama-embed-nemotron-8b")
# MODEL_ID=("nvidia_llama-embed-nemotron-8b")
MODEL_NAME=("intfloat/multilingual-e5-large")
MODEL_ID=("intfloat_multilingual-e5-large")
# MODEL_NAME=("intfloat/multilingual-e5-large" "mixedbread-ai/mxbai-embed-large-v1")
# MODEL_ID=("intfloat_multilingual-e5-large" "mixedbread-ai_mxbai-embed-large-v1")
TRAIN_BATCH_SIZE=("128")
# POOLER_OPTIONS=("avg" "max" "cls")
POOLER_OPTIONS=("cls")
# POOLER_OPTIONS=("last")
# POOLER_OPTIONS=("avg")
FREEZE_ENCODER_OPTIONS=("false")
# FREEZE_ENCODER_OPTIONS=("true")
# LEARNING_RATES=("5e-4" "1e-4" "5e-5")
LEARNING_RATES=("5e-5")
# Train all 4 classifier configurations sequentially
CLASSIFIER_TYPES=("GPT" "nGPT")
# CLASSIFIER_TYPES=("nGPT")
# SEED_VALUES=("42")
SEED_VALUES=("42")
# SEED_VALUES=("42" "43" "44" "45" "46")
LOSS_FUNCTION_VALUES=("SINCERE" "CircularCSE" "SoftCSE")
# LOSS_FUNCTION_VALUES=("SINCERE" "SoftCSE")
# LOSS_FUNCTION_VALUES=("SINCERE")
# LOSS_FUNCTION_VALUES=("CircularCSE")
# LOSS_FUNCTION_VALUES=("SoftCSE")
# LOSS_FUNCTION_VALUES=("softCSE")
# USE_ANGLE_MAP=("true" "false")
USE_ANGLE_MAP=("true")
LOG_OF_SUM=("false")
# INPUT_DIMS=("16" "32" "64" "128" "256" "512" "768")
# INPUT_DIMS=("2" "4" "8" "16" "32" "64")
# INPUT_DIMS=("16" "32" "64")
# INPUT_DIMS=("2")
# INPUT_DIMS=("4096")
# INPUT_DIMS=("3072")
# INPUT_DIMS=("2560")
INPUT_DIMS=("1024")
MARGIN=("0.0")

# Dataset configurations: dataset_prefix, train_file, valid_file, test_file, project_suffix
DATASETS=(
    "Emolit12:emolit_12labels_train.csv:emolit_12labels_test.csv:emolit_12labels_test.csv:emolit12"
    # "Synthesis12:mixed-synthesis_12labels_train.csv:mixed-synthesis_12labels_test.csv:mixed-synthesis_12labels_test.csv:synthesis12"
    # "Empathetic8:empathetic_8labels_train.csv:empathetic_8labels_test.csv:empathetic_8labels_test.csv:empathetic8"
    # "SuperEmotion9:super-emotion_9labels_train.csv:super-emotion_9labels_test.csv:super-emotion_9labels_test.csv:super-emotion9"
    # "Emolit-fr:emolit-fr_12labels_train.csv:emolit-fr_12labels_test.csv:emolit-fr_12labels_test.csv:emolit-fr"
    # "Emolit-it:emolit-it_12labels_train.csv:emolit-it_12labels_test.csv:emolit-it_12labels_test.csv:emolit-it"
    # "Emolit-nl:emolit-nl_12labels_train.csv:emolit-nl_12labels_test.csv:emolit-nl_12labels_test.csv:emolit-nl"
    # "Emolit2:emolit_2labels_train.csv:emolit_2labels_test.csv:emolit_2labels_test.csv:emolit2"
    # "Emolit3:emolit_3labels_train.csv:emolit_3labels_test.csv:emolit_3labels_test.csv:emolit3"
    # "Emolit4:emolit_4labels_train.csv:emolit_4labels_test.csv:emolit_4labels_test.csv:emolit4"
    # "Emolit6:emolit_6labels_train.csv:emolit_6labels_test.csv:emolit_6labels_test.csv:emolit6"
    # "Emolit24:emolit_24labels_train.csv:emolit_24labels_test.csv:emolit_24labels_test.csv:emolit24"    
)

BASE_WANDB_PROJECT=${WANDB_PROJECT:-predimension_reduction}

sanitize_token() {
  local value="$1"
  value="${value//./p}"
  value="${value//-/m}"
  value="${value// /}"
  echo "${value}"
}

for dataset_info in "${DATASETS[@]}"; do
  IFS=':' read -r dataset_prefix train_file valid_file test_file project_suffix <<< "${dataset_info}"
  for i in "${!MODEL_NAME[@]}"; do
    model="${MODEL_NAME[$i]}"
    model_id="${MODEL_ID[$i]}"
    for train_batch_size in "${TRAIN_BATCH_SIZE[@]}"; do
      for seed in "${SEED_VALUES[@]}"; do
        for pooler in "${POOLER_OPTIONS[@]}"; do
          pooler_token=$(sanitize_token "${pooler}")
          for freeze in "${FREEZE_ENCODER_OPTIONS[@]}"; do
            for lr in "${LEARNING_RATES[@]}"; do
              for loss_function in "${LOSS_FUNCTION_VALUES[@]}"; do
                for classifier in "${CLASSIFIER_TYPES[@]}"; do
                  for input_dim in "${INPUT_DIMS[@]}"; do
                    for use_angle_map in "${USE_ANGLE_MAP[@]}"; do
                      for log_of_sum in "${LOG_OF_SUM[@]}"; do
                        for margin in "${MARGIN[@]}"; do
                          lr_token=$(sanitize_token "${lr}")
                          loss_function_token=$(sanitize_token "${loss_function}")
                          project_name="loss_${loss_function_token}_clf_${classifier}_size:${project_suffix}_indim:${input_dim}_angmap:${use_angle_map}_logofsum:${log_of_sum}_margin:${margin}_seed:${seed}"
                          wandb_project="${BASE_WANDB_PROJECT}_${project_suffix}"
                          echo "=== Running: DATASET=${dataset_prefix}, POOLER=${pooler}, FREEZE_ENCODER=${freeze}, LR=${lr}, LOSS_FUNCTION=${loss_function}, CLASSIFIER=${classifier}, SEED=${seed} ===, INPUT_DIM=${input_dim}"

                          TRAIN_FILE="${train_file}" \
                          VALID_FILE="${valid_file}" \
                          TEST_FILE="${test_file}" \
                          MODEL_NAME="${model}" \
                          MODEL_ID="${model_id}" \
                          TRAIN_BATCH_SIZE="${train_batch_size}" \
                          POOLER_TYPE="${pooler}" \
                          FREEZE_ENCODER="${freeze}" \
                          LR="${lr}" \
                          LOSS_FUNCTION="${loss_function}" \
                          CLASSIFIER_TYPE="${classifier}" \
                          WANDB_PROJECT="${wandb_project}" \
                          WANDB_PROJECT_NAME="${project_name}" \
                          SEED="${seed}" \
                          INPUT_DIM="${input_dim}" \
                          LOG_OF_SUM="${log_of_sum}" \
                          MARGIN="${margin}" \
                          USE_ANGLE_MAP="${use_angle_map}" \
                          bash "${TRAIN_SCRIPT}"
                        done
                      done
                    done
                  done
                done
              done
            done
          done
        done
      done
    done
  done
done
